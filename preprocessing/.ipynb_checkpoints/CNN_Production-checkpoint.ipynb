{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook introduces many CNN for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noble-civilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ama/audibeal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ama/audibeal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ama/audibeal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import library\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gensim\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import gc\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-speed",
   "metadata": {},
   "source": [
    "# 1) Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "likely-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_base, stemming=False, lemmatisation=False):\n",
    "    \"\"\" Preprocessing Data, remove punctuation, figures\n",
    "    and add lemmatisation or stemming and tokenization\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_base: numpy.ndarray\n",
    "    stemming: boolean, default: False\n",
    "    lemmatisation: boolean, default: False\n",
    "    Returns\n",
    "    -------\n",
    "    maximum: int\n",
    "        Represents the maximum length of sentence.\n",
    "    dataset_tokenization: list:\n",
    "    \"\"\"\n",
    "    dataset_tokenization = []\n",
    "    maximum = 0\n",
    "    for document in data_base:\n",
    "        document = document.lower()\n",
    "        token = word_tokenize(document)\n",
    "        token = [word for word in token if word.isalpha()]\n",
    "\n",
    "        # remove non-relevant word\n",
    "        new_word = \\\n",
    "            [word for word in token if word not in stopwords.words('english')]\n",
    "\n",
    "        # Stemming part\n",
    "        if stemming:\n",
    "            porter = PorterStemmer()\n",
    "            new_word = [porter.stem(word) for word in new_word]\n",
    "\n",
    "        # Lemmatisation part\n",
    "        if lemmatisation:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            new_word = [lemmatizer.lemmatize(word) for word in new_word]\n",
    "\n",
    "        # Update Variables\n",
    "        maximum = max(maximum, len(new_word))\n",
    "        dataset_tokenization.append(new_word)\n",
    "    return maximum, dataset_tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "convinced-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dic_voc(voc_known):\n",
    "    \"\"\" Create a dictionnary of important words\n",
    "    Parameters\n",
    "    ----------\n",
    "    voc_known: mapping dictionnary\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary: list\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    for index, token in enumerate(voc_known):\n",
    "        vocabulary[token] = index\n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dramatic-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_matrix_emb(vocabulary, model):\n",
    "    \"\"\"Create Matrix for word embedding\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabulary : mapping dictionnary\n",
    "        Represent interesting vocabulary\n",
    "    model:\n",
    "        Represents a word2Vec model\n",
    "    Returns\n",
    "    -------\n",
    "    weights_matrix: numpy.ndarray\n",
    "        Represents weights for nn.embedding\n",
    "    final_dico: mapping dictionnary\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    matrix_len = len(vocabulary)\n",
    "    len_emb = len(model['law'])\n",
    "    know_word = 0\n",
    "    shift = 2\n",
    "    weights_matrix = np.zeros((matrix_len + shift, len_emb))\n",
    "    final_dico = {}\n",
    "    # clef space : index = 0\n",
    "    # clef unknwn: index = 1\n",
    "\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        if word in model:\n",
    "            weights_matrix[i + shift] = model[word]\n",
    "            final_dico[word] = i + shift\n",
    "            know_word += 1\n",
    "        else:\n",
    "            weights_matrix[i + shift] = \\\n",
    "                np.random.normal(scale=0.6, size=(len_emb, ))\n",
    "            final_dico[word] = i + shift\n",
    "    # Return final dico/ and matrix weight corresponding\n",
    "    print('{0}% are known by pre-trained model'.format(know_word / matrix_len))\n",
    "    return weights_matrix, final_dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "confidential-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, maximum, final_dico, remove_unknown_word=False):\n",
    "    \"\"\" Transform our data into good format for training\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy.ndarray\n",
    "        X is the word tokenize to change into number\n",
    "    maximum: int\n",
    "        Maximum dimension\n",
    "    model:\n",
    "        Represents a word2Vec model\n",
    "    Returns\n",
    "    -------\n",
    "    data_: tensor\n",
    "    \"\"\"\n",
    "    data_ = torch.zeros(len(data), maximum, dtype=torch.long)\n",
    "    for id_text, text in enumerate(data):\n",
    "        id_token = 0\n",
    "        for _, token in enumerate(text):\n",
    "            if id_token == maximum:\n",
    "                break\n",
    "            if token in final_dico:\n",
    "                data_[id_text, id_token] = final_dico[token]\n",
    "                id_token += 1\n",
    "            else:\n",
    "                if not(remove_unknown_word):\n",
    "                    data_[id_text, id_token] = 1\n",
    "                    id_token += 1\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "instrumental-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(data):\n",
    "    \"\"\" Frequency of words in our datatset\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy.ndarray\n",
    "        data is the word tokenize\n",
    "    Returns\n",
    "    -------\n",
    "    dico_freq: mapping dictionnary\n",
    "    \"\"\"\n",
    "    dico_freq = {}\n",
    "    for text in data:\n",
    "        current_dico = {}\n",
    "        for token in text:\n",
    "            if token in dico_freq and token not in current_dico:\n",
    "                dico_freq[token] += 1\n",
    "                current_dico[token] = 0\n",
    "            elif token not in current_dico:\n",
    "                dico_freq[token] = 1\n",
    "            else:\n",
    "                pass\n",
    "    return dico_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inner-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequence_keep(data, thresh=5):\n",
    "    \"\"\" Keep only words with hight frequence\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy.ndarray\n",
    "        data is the word tokenize\n",
    "    thresh: int, default: 5\n",
    "    Returns\n",
    "    -------\n",
    "    _ : list(char)\n",
    "    \"\"\"\n",
    "    dico_feq = frequency(data)\n",
    "    voc_known = [element for element in dico_feq if dico_feq[element] >= thresh]\n",
    "    vocabulary = {}\n",
    "    for index, token in enumerate(voc_known):\n",
    "        vocabulary[token] = index\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "electric-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cuda():\n",
    "    \"\"\"\n",
    "    Remove Useless element on GPU\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "attached-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\" Evalute our model with f1, recall precision\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: torch/numpy.ndarray\n",
    "    y_pred: torch/numpy.ndarray\n",
    "    dic_index: dictionnary\n",
    "    average: String\n",
    "        Represent type of f1\n",
    "    \"\"\"\n",
    "    f1 = f1_score(y_true, y_pred, average=average)\n",
    "    precision = precision_score(y_true, y_pred, average=average)\n",
    "    recall = recall_score(y_true, y_pred, average=average)\n",
    "    accuracy = sum(y_pred == y_true) / len(y_pred)\n",
    "    print('Accuracy: {0} \\n F1: {1} \\n precision : {2} \\n recall : {3} '\n",
    "          .format(accuracy, f1, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-magazine",
   "metadata": {},
   "source": [
    "# 2) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "funded-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    \"\"\"Create Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        if len(self.x) != len(self.y):\n",
    "            raise Exception(\"The length of X does not match the length of Y\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Note that this isn't randomly selecting.\n",
    "        # It's a simple get a single item that represents an x and y\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-filling",
   "metadata": {},
   "source": [
    "# 3) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "average-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnKim(nn.Module):\n",
    "    \"\"\" Kim CNN\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_weight: torch.float\n",
    "    len_sentence: int\n",
    "    kernel_sizes: list\n",
    "    out_channel_len: int\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 matrix_weight,\n",
    "                 len_sentence,\n",
    "                 kernel_sizes=[3, 4, 5],\n",
    "                 out_channel_len=100):\n",
    "        super(CnnKim, self).__init__()\n",
    "        self.embbeding_1 = nn.Embedding(matrix_weight.shape[0],\n",
    "                                        matrix_weight.shape[1])\n",
    "        self.embbeding_1.weight.data = matrix_weight\n",
    "        self.embbeding_1.weight.requires_grad = False\n",
    "        self.embbeding_2 = nn.Embedding(matrix_weight.shape[0],\n",
    "                                        matrix_weight.shape[1])\n",
    "        self.embbeding_2.weight.data = torch.clone(matrix_weight)\n",
    "        self.embbeding_2.weight.requires_grad = True\n",
    "\n",
    "        block = []\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv1d = nn.Conv2d(in_channels=2,\n",
    "                               out_channels=out_channel_len,\n",
    "                               kernel_size=(kernel_size,\n",
    "                                            matrix_weight.shape[1]))\n",
    "            component = nn.Sequential(\n",
    "                conv1d,\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(2),\n",
    "                nn.MaxPool1d(kernel_size=len_sentence, ceil_mode=True))\n",
    "            block.append(component)\n",
    "        self.block = nn.ModuleList(block)\n",
    "        self.linear = nn.Linear(out_channel_len * len(kernel_sizes), 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.embbeding_1(x)\n",
    "        x2 = self.embbeding_2(x)\n",
    "        x = torch.cat((x1.unsqueeze(1), x2.unsqueeze(1)), 1)\n",
    "        x_list = [conv_block(x) for conv_block in self.block]\n",
    "        x = torch.cat(x_list, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "characteristic-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XmlCnn(nn.Module):\n",
    "    \"\"\" Kim CNN\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix_weight: torch.float\n",
    "    len_sentence: int\n",
    "    kernel_sizes: list\n",
    "    nb_max_pool: int\n",
    "    nb_output: int\n",
    "    \"\"\"\n",
    "    def __init__(self, matrix_weight,\n",
    "                 len_sentence, kernel_sizes=[2, 4, 8],\n",
    "                 nb_max_pool=10,\n",
    "                 nb_output=32):\n",
    "\n",
    "        super(XmlCnn, self).__init__()\n",
    "        self.nb_max_pool = nb_max_pool\n",
    "        self.nb_output = nb_output\n",
    "        self.embbeding_1 = nn.Embedding(matrix_weight.shape[0],\n",
    "                                        matrix_weight.shape[1])\n",
    "        self.embbeding_1.weight.data = matrix_weight\n",
    "        self.embbeding_1.weight.requires_grad = False\n",
    "\n",
    "        self.embbeding_2 = nn.Embedding(matrix_weight.shape[0],\n",
    "                                        matrix_weight.shape[1])\n",
    "        self.embbeding_2.weight.data = torch.clone(matrix_weight)\n",
    "        self.embbeding_2.weight.requires_grad = True\n",
    "\n",
    "        block = []\n",
    "        for kernel_size in kernel_sizes:\n",
    "            maxpool_size = int((len_sentence - kernel_size + 1)/nb_max_pool) + 1\n",
    "            conv1d = nn.Conv2d(in_channels=2,\n",
    "                               out_channels=self.nb_output,\n",
    "                               kernel_size=(kernel_size,\n",
    "                                            matrix_weight.shape[1]))\n",
    "            component = nn.Sequential(\n",
    "                conv1d,\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(2),\n",
    "                nn.ConstantPad1d((0, nb_max_pool), 0),\n",
    "                nn.MaxPool1d(kernel_size=maxpool_size, stride=maxpool_size))\n",
    "            block.append(component)\n",
    "        self.block = nn.ModuleList(block)\n",
    "\n",
    "        self.Bottleneck = nn.Linear(nb_output * nb_max_pool * len(kernel_sizes),\n",
    "                                    512)\n",
    "        self.linear = nn.Linear(512, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.embbeding_1(x)\n",
    "        x2 = self.embbeding_2(x)\n",
    "        x = torch.cat((x1.unsqueeze(1), x2.unsqueeze(1)), 1)\n",
    "        x_list = [conv_block(x) for conv_block in self.block]\n",
    "        x = torch.cat(x_list, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.Bottleneck(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "iraqi-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, channel_size):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.channel_size = channel_size\n",
    "        self.maxpool = nn.Sequential(\n",
    "            nn.ConstantPad1d(padding=(0, 1), value=0),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size,\n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size,\n",
    "                      kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shortcut = self.maxpool(x)\n",
    "        x = self.conv(x_shortcut)\n",
    "        x = x + x_shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class DPCNN(nn.Module):\n",
    "    def __init__(self, matrix_weight, len_seq, channel_dim=200):\n",
    "        super(DPCNN, self).__init__()\n",
    "        self.embbeding_1 = nn.Embedding(matrix_weight.shape[0],\n",
    "                                        matrix_weight.shape[1])\n",
    "        self.embbeding_1.weight.data = matrix_weight\n",
    "        self.embbeding_1.weight.requires_grad = False\n",
    "        self.embbeding_2 = nn.Embedding(matrix_weight.shape[0],\n",
    "                                        matrix_weight.shape[1])\n",
    "        self.embbeding_2.weight.data = torch.clone(matrix_weight)\n",
    "        self.embbeding_2.weight.requires_grad = True\n",
    "        # region embedding\n",
    "        self.region_embedding = nn.Sequential(\n",
    "            nn.Conv2d(2, channel_dim, kernel_size=(3, 200), padding=(1, 0)),\n",
    "            nn.Flatten(2),\n",
    "            nn.BatchNorm1d(channel_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=channel_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(channel_dim, channel_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_features=channel_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(channel_dim, channel_dim, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.num_seq = len_seq\n",
    "        resnet_block_list = []\n",
    "        while (self.num_seq > 2):\n",
    "            resnet_block_list.append(ResnetBlock(channel_dim))\n",
    "            self.num_seq = self.num_seq // 2\n",
    "\n",
    "        self.resnet_layer = nn.Sequential(*resnet_block_list)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel_dim * self.num_seq, 2),\n",
    "            nn.BatchNorm1d(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.embbeding_1(x)\n",
    "        x2 = self.embbeding_2(x)\n",
    "        x = torch.cat((x1.unsqueeze(1), x2.unsqueeze(1)), 1)\n",
    "        x = self.region_embedding(x)\n",
    "        x = self.conv_block(x)\n",
    "        x = self.resnet_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "frank-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnPart(nn.Module):\n",
    "    def __init__(self,\n",
    "                 matrix_weight,\n",
    "                 len_sentence,\n",
    "                 kernel_sizes=[1, 2, 3],\n",
    "                 out_channel_len=32):\n",
    "        super(CnnPart, self).__init__()\n",
    "        self.embbeding_1 = nn.Embedding(matrix_weight.shape[0],\n",
    "                                        matrix_weight.shape[1])\n",
    "        self.embbeding_1.weight.data = matrix_weight\n",
    "        self.embbeding_1.weight.requires_grad = False\n",
    "        self.embbeding_2 = nn.Embedding(matrix_weight.shape[0],\n",
    "                                        matrix_weight.shape[1])\n",
    "        self.embbeding_2.weight.data = torch.clone(matrix_weight)\n",
    "        self.embbeding_2.weight.requires_grad = True\n",
    "\n",
    "        block = []\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv1d = nn.Conv2d(in_channels=2,\n",
    "                               out_channels=out_channel_len,\n",
    "                               kernel_size=(kernel_size,\n",
    "                                            matrix_weight.shape[1]))\n",
    "            component = nn.Sequential(\n",
    "                conv1d,\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(2),\n",
    "                nn.MaxPool1d(kernel_size=len_sentence, ceil_mode=True))\n",
    "            block.append(component)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.block = nn.ModuleList(block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.embbeding_1(x)\n",
    "        x2 = self.embbeding_2(x)\n",
    "        x = torch.cat((x1.unsqueeze(1), x2.unsqueeze(1)), 1)\n",
    "        x = self.dropout(x)\n",
    "        x_list = [conv_block(x) for conv_block in self.block]\n",
    "        x = torch.cat(x_list, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tamil-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLstmAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim):\n",
    "        super(BiLstmAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_size,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        hn = hn.reshape(hn.shape[1], -1)\n",
    "        return output, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "solar-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention_1(nn.Module):\n",
    "    def __init__(self, init, dim):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.attention_key = nn.Linear(init, dim, bias=False)\n",
    "        self.attention_query = nn.Linear(init, dim, bias=False)\n",
    "        self.v = nn.Linear(dim, 1)\n",
    "        self.attention_vec = nn.Linear(2 * init, 128, bias=False)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.activation_weight = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores =\\\n",
    "            self.v(self.activation(self.attention_query(query).unsqueeze(1) + \n",
    "                                   self.attention_key(keys)))\n",
    "        scores = scores.reshape(scores.shape[0], -1)\n",
    "        attention_weight = self.activation_weight(scores)\n",
    "        vector_context = torch.bmm(attention_weight.unsqueeze(1), keys)\n",
    "        pred = torch.cat((vector_context.squeeze(1), query), 1)\n",
    "        attention_vector = self.activation(self.attention_vec(pred))\n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "oriental-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttentio_1(nn.Module):\n",
    "    def __init__(self, init, dim):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.attention_key = nn.Linear(init, dim, bias=False)\n",
    "        self.attention_query = nn.Linear(3, dim, bias=False)\n",
    "        self.context_weight = nn.Parameter(torch.Tensor(int(dim), 1))\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linear = nn.Linear(init, int(init/2))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, hn, x):\n",
    "        scores = self.activation(self.attention_query(x) + self.attention_key(hn))\n",
    "        scores = scores @ self.context_weight\n",
    "        weight = self.softmax(scores).permute(0, 2, 1)\n",
    "        attention_vector = torch.bmm(weight, hn)\n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "subject-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, init, dim):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.context_weight = nn.Linear(init, 1, bias=False)\n",
    "        self.linear = nn.Linear(init, init)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scale = 1.0/np.sqrt(init)\n",
    "        \n",
    "    def forward(self, hn, x):\n",
    "        u = self.linear(hn)\n",
    "        weight = self.softmax(self.context_weight(u).mul_(self.scale)).permute(0, 2, 1)\n",
    "        attention_vector = torch.bmm(weight, hn) \n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "conceptual-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention_X(nn.Module):\n",
    "    def __init__(self, init, dim):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.multihead = nn.MultiheadAttention(init, 8)\n",
    "        self.query = nn.Parameter(torch.empty(4, init).normal_(mean=0.1,std=0.5))\n",
    "        \n",
    "    def forward(self, hn, x):\n",
    "        hn = hn.permute(1, 0, 2)\n",
    "        query = self.query.repeat(hn.shape[1],1 , 1)\n",
    "        query = query.permute(1, 0, 2)\n",
    "        attention_vector, _ = self.multihead(query, hn, hn )\n",
    "        attention_vector = attention_vector.permute(1, 0, 2)\n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "significant-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnLstm(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cnn,\n",
    "                 lstm,\n",
    "                 attention,\n",
    "                hidden_size):\n",
    "        super(CnnLstm,self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.LSTM = lstm\n",
    "        self.attention = attention\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.predict = nn.Linear(2*hidden_size, 2)#4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        output, hn = self.LSTM(x)\n",
    "        attention_vector = self.attention(output, x)\n",
    "        attention_vector = attention_vector.contiguous() \n",
    "        attention_vector = attention_vector.view(attention_vector.shape[0], -1)\n",
    "        attention_vector = self.dropout(attention_vector)\n",
    "        return self.predict(attention_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-personal",
   "metadata": {},
   "source": [
    "# 4) Call Backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "higher-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallBack():\n",
    "    def __init__(self, lr, factor=0.1, patient=2, path='valider.pth'):\n",
    "        self.memory_val = []\n",
    "        self.path = path\n",
    "        \n",
    "    def add_val(self, mean_loss):\n",
    "        self.memory_val.append(mean_loss)\n",
    "\n",
    "    def save_best(self, model):\n",
    "        if self.memory_val[-1] == min(self.memory_val):\n",
    "            print('====== Save New model ===== ')\n",
    "            print(self.memory_val)\n",
    "            torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "subsequent-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_callback(model, trainer, valider, name_save, w=torch.tensor([9., 1.]).cuda(), weight_decay=0, epochs=10, lr=10e-3):\n",
    "    criterion = nn.CrossEntropyLoss(weight=w)\n",
    "    callback = CallBack(lr, path=name_save + '.pth')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 2)\n",
    "    for epoch in range(epochs):  \n",
    "        running_loss = 0.0\n",
    "        mean = 0.0\n",
    "        nb = 0\n",
    "        for i, data in enumerate(trainer):\n",
    "            inputs, labels = data\n",
    "            if torch.cuda.is_available():\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Affichage training part results\n",
    "            if i % 50 == 49:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 50))\n",
    "                nb += 1\n",
    "                mean += running_loss/50\n",
    "                running_loss = 0.0\n",
    "        print('---- Mean loss = %.3f ----' % (mean / nb), end='')\n",
    "        \n",
    "        # Work on validation set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            for i, data in enumerate(valider):\n",
    "                inputs, labels = data\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss_val += criterion(outputs, labels.long())\n",
    "            print('---- Loss Val %.3f----' % loss_val)\n",
    "            callback.add_val(loss_val)\n",
    "            callback.save_best(model)\n",
    "        model.train()\n",
    "        curr_lr = optimizer.param_groups[0]['lr']\n",
    "        print('=== Current lr: %.3f===' % curr_lr)\n",
    "        scheduler.step(loss_val)\n",
    "        \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-opposition",
   "metadata": {},
   "source": [
    "# 5) Download and preprocessing our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dried-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmatization = False\n",
    "lemmatization = True\n",
    "truncated = True\n",
    "remove_unknow_word = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "played-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"data2word.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "elementary-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.loc['EN_train_Anon'][\"Fact\"].values\n",
    "Y_train = data.loc['EN_train_Anon'][\"Violation\"].values\n",
    "\n",
    "X_val = data.loc['EN_dev_Anon'][\"Fact\"].values\n",
    "Y_val = data.loc['EN_dev_Anon'][\"Violation\"].values\n",
    "\n",
    "X_test = data.loc['EN_test_Anon'][\"Fact\"].values\n",
    "Y_test = data.loc['EN_test_Anon'][\"Violation\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rolled-coast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "maximum, X_train = preprocessing(X_train, stemming=stemmatization, lemmatisation=lemmatization)\n",
    "print(\"1\")\n",
    "dic_known = frequence_keep(X_train)\n",
    "print(\"2\")\n",
    "maximum_1, X_val = preprocessing(X_val, stemming=stemmatization, lemmatisation=lemmatization)\n",
    "maximum_2, X_test = preprocessing(X_test, stemming=stemmatization, lemmatisation=lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "standard-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eligible-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truncated:\n",
    "    max_length = maximum\n",
    "else:\n",
    "    max_length = max(maximum, maximum_1, maximum_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "collectible-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('Law2Vec.200d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "found-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb word in my dictionnary  16597\n"
     ]
    }
   ],
   "source": [
    "dictionnary = create_dic_voc(dic_known)\n",
    "print('nb word in my dictionnary ', len(dictionnary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "separated-sword",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8524432126287883% are known by pre-trained model\n"
     ]
    }
   ],
   "source": [
    "# Create matrix of weight for first embedding\n",
    "weight_matrix, final_dico = weight_matrix_emb(dictionnary, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "surgical-prior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 good\n",
      "Step 2 good\n",
      "Step 3 good\n"
     ]
    }
   ],
   "source": [
    "# ncoding_data\n",
    "X_train = transform_data(X_train, max_length, final_dico, remove_unknow_word)\n",
    "print(\"Step 1 good\")\n",
    "X_test = transform_data(X_test, max_length, final_dico, remove_unknow_word)\n",
    "print(\"Step 2 good\")\n",
    "X_val = transform_data(X_val, max_length, final_dico, remove_unknow_word)\n",
    "print(\"Step 3 good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "controlling-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good format:\n",
    "Y_train = torch.tensor(Y_train).float()\n",
    "Y_test = torch.tensor(Y_test).float()\n",
    "Y_val = torch.tensor(Y_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "imperial-lexington",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f91168944b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_save\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_train'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_save\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_test'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_save\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'X_val'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save' is not defined"
     ]
    }
   ],
   "source": [
    "if save:\n",
    "    torch.save(X_train, name_save + 'X_train' + '.pt')\n",
    "    torch.save(X_test, name_save + 'X_test' + '.pt')\n",
    "    torch.save(X_val, name_save + 'X_val' + '.pt')\n",
    "\n",
    "    torch.save(Y_train, name_save + 'Y_train' + '.pt')\n",
    "    torch.save(Y_test, name_save + 'Y_test' + '.pt')\n",
    "    torch.save(Y_val, name_save + 'Y_val' + '.pt')\n",
    "    np.save(name_save + 'weight.npy', weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_choose = 2\n",
    "save = True\n",
    "first_folder = 'data/'\n",
    "possible_path = ['summary', 'keep_relevant', 'remove_relevant']\n",
    "\n",
    "#Test\n",
    "for element in possible_path:\n",
    "    if not(os.path.exists(first_folder + element)):\n",
    "        raise NameError(element)\n",
    "\n",
    "print('You have choose the data: ', possible_path[data_choose])\n",
    "\n",
    "name_save = first_folder + possible_path[data_choose] + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "valuable-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    X_train = torch.load(name_download + 'X_train' + '.pt')\n",
    "    X_test = torch.load(name_download + 'X_test' + '.pt')\n",
    "    X_val = torch.load(name_download + 'X_val' + '.pt')\n",
    "\n",
    "    Y_train = torch.load(name_download + 'Y_train' + '.pt')\n",
    "    Y_test = torch.load(name_download + 'Y_test' + '.pt')\n",
    "    Y_val = torch.load(name_download + 'Y_val' + '.pt')\n",
    "\n",
    "    max_length = X_train.shape[1]\n",
    "    weight_matrix = np.load(name_download + 'weight.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "tight-kingston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have choose the data:  keep_relevant\n"
     ]
    }
   ],
   "source": [
    "data_download = 1\n",
    "download = True\n",
    "first_folder = 'data/'\n",
    "possible_path = ['summary', 'keep_relevant', 'remove_relevant']\n",
    "\n",
    "#Test\n",
    "for element in possible_path:\n",
    "    if not(os.path.exists(first_folder + element)):\n",
    "        raise NameError(element)\n",
    "\n",
    "print('You have choose the data: ', possible_path[data_download])\n",
    "\n",
    "name_download = first_folder + possible_path[data_download] + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "concrete-india",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22188"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "classified-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 1])\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       device='cuda:0', grad_fn=<PermuteBackward>)\n",
      "torch.Size([16, 64, 1])\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       device='cuda:0', grad_fn=<PermuteBackward>)\n",
      "torch.Size([16, 64, 1])\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       device='cuda:0', grad_fn=<PermuteBackward>)\n",
      "torch.Size([16, 64, 1])\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       device='cuda:0', grad_fn=<PermuteBackward>)\n",
      "torch.Size([16, 64, 1])\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       device='cuda:0', grad_fn=<PermuteBackward>)\n",
      "torch.Size([16, 64, 1])\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       device='cuda:0', grad_fn=<PermuteBackward>)\n",
      "torch.Size([16, 64, 1])\n",
      "tensor([[[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]],\n",
      "\n",
      "        [[0.0156, 0.0156, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
      "       device='cuda:0', grad_fn=<PermuteBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-4c01a24ac5c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                    lr=lr)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-ca6b0a21e06c>\u001b[0m in \u001b[0;36mtrain_callback\u001b[0;34m(model, trainer, valider, name_save, w, weight_decay, epochs, lr)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mon_virtualenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mon_virtualenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs = 15\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "kernel_sizes = [3, 4, 5]\n",
    "hidden_size = 64\n",
    "out_channel = 64\n",
    "\n",
    "for numero in range(2):\n",
    "    clean_cuda()\n",
    "    name = 'bestModel/CnnLstm/' + str(numero) + '_2'\n",
    "    c = CnnPart(torch.from_numpy(weight_matrix).float(),\n",
    "                max_length,\n",
    "                kernel_sizes,\n",
    "                out_channel_len=out_channel)\n",
    "\n",
    "    lstm = BiLstmAttention(hidden_size=hidden_size,\n",
    "                           embedding_dim=len(kernel_sizes))\n",
    "\n",
    "    attention = BahdanauAttention(2 * hidden_size,\n",
    "                                  hidden_size)\n",
    "\n",
    "    cnn = CnnLstm(c,\n",
    "                  lstm,\n",
    "                  attention,\n",
    "                 hidden_size)\n",
    "    cnn = cnn.to(device)\n",
    "    # Create trainer\n",
    "    trainer = list(DataLoader(DataSet(X_train, Y_train),\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True))\n",
    "    valider = list(DataLoader(DataSet(X_val, Y_val),\n",
    "                              batch_size=1,\n",
    "                              shuffle=False))\n",
    "    # train our model\n",
    "\n",
    "    train_callback(cnn,\n",
    "                   trainer,\n",
    "                   valider,\n",
    "                   name_save=name,\n",
    "                   w=None,\n",
    "                   weight_decay=1e-4,\n",
    "                   epochs=epochs,\n",
    "                   lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "mineral-speech",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "normal() received an invalid combination of arguments - got (float, int), but expected one of:\n * (Tensor mean, Tensor std, torch.Generator generator, Tensor out)\n * (Tensor mean, float std, torch.Generator generator, Tensor out)\n * (float mean, Tensor std, torch.Generator generator, Tensor out)\n * (float mean, float std, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-e45242d648e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: normal() received an invalid combination of arguments - got (float, int), but expected one of:\n * (Tensor mean, Tensor std, torch.Generator generator, Tensor out)\n * (Tensor mean, float std, torch.Generator generator, Tensor out)\n * (float mean, Tensor std, torch.Generator generator, Tensor out)\n * (float mean, float std, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "a = torch.normal(0.1, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accomplished-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_preds(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "            (all_preds, preds.cpu()), dim=0)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "spatial-female",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CnnLstm(\n",
       "  (cnn): CnnPart(\n",
       "    (embbeding_1): Embedding(16599, 200)\n",
       "    (embbeding_2): Embedding(16599, 200)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (block): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(2, 64, kernel_size=(3, 200), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Flatten()\n",
       "        (3): MaxPool1d(kernel_size=22188, stride=22188, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(2, 64, kernel_size=(4, 200), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Flatten()\n",
       "        (3): MaxPool1d(kernel_size=22188, stride=22188, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(2, 64, kernel_size=(5, 200), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Flatten()\n",
       "        (3): MaxPool1d(kernel_size=22188, stride=22188, padding=0, dilation=1, ceil_mode=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (LSTM): BiLstmAttention(\n",
       "    (lstm): LSTM(3, 64, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (attention): BahdanauAttention(\n",
       "    (context_weight): Linear(in_features=128, out_features=1, bias=False)\n",
       "    (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (predict): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs = 15\n",
    "lr = 1e-4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "kernel_sizes = [3, 4, 5]\n",
    "hidden_size = 64\n",
    "out_channel = 64\n",
    "numero = 2\n",
    "name = 'bestModel/CnnLstm/' + str(numero) + '_2' + '.pth'\n",
    "\n",
    "c = CnnPart(torch.from_numpy(weight_matrix).float(),\n",
    "                max_length,\n",
    "                kernel_sizes,\n",
    "                out_channel_len=out_channel)\n",
    "\n",
    "\n",
    "\n",
    "lstm = BiLstmAttention(hidden_size=hidden_size,embedding_dim=len(kernel_sizes))\n",
    "\n",
    "attention = BahdanauAttention(2 * hidden_size,hidden_size)\n",
    "\n",
    "cnn = CnnLstm(c,lstm,attention,hidden_size)\n",
    "\n",
    "cnn.load_state_dict(torch.load(name))\n",
    "cnn.to(device)\n",
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "thermal-journal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8549032688458973 \n",
      " F1: 0.8243123436700753 \n",
      " precision : 0.8727861626616813 \n",
      " recall : 0.8033418819655522 \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = iter(DataLoader(DataSet(X_test, Y_test),\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False))\n",
    "    y_pred = get_all_preds(cnn, prediction_loader)\n",
    "proba, predicted = torch.max(y_pred, 1)\n",
    "new_score(Y_test.numpy(), predicted.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-piano",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.715\n",
      "[1,   100] loss: 0.641\n",
      "[1,   150] loss: 0.574\n",
      "[1,   200] loss: 0.513\n",
      "[1,   250] loss: 0.494\n",
      "[1,   300] loss: 0.440\n",
      "[1,   350] loss: 0.433\n",
      "[1,   400] loss: 0.403\n",
      "---- Mean loss = 0.527 -------- Loss Val 545.273----\n",
      "====== Save New model ===== \n",
      "[tensor(545.2727, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[2,    50] loss: 0.456\n",
      "[2,   100] loss: 0.410\n",
      "[2,   150] loss: 0.409\n",
      "[2,   200] loss: 0.380\n",
      "[2,   250] loss: 0.397\n",
      "[2,   300] loss: 0.376\n",
      "[2,   350] loss: 0.361\n",
      "[2,   400] loss: 0.358\n",
      "---- Mean loss = 0.393 -------- Loss Val 501.778----\n",
      "====== Save New model ===== \n",
      "[tensor(545.2727, device='cuda:0'), tensor(501.7780, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[3,    50] loss: 0.408\n",
      "[3,   100] loss: 0.373\n",
      "[3,   150] loss: 0.386\n",
      "[3,   200] loss: 0.352\n",
      "[3,   250] loss: 0.373\n",
      "[3,   300] loss: 0.345\n",
      "[3,   350] loss: 0.328\n",
      "[3,   400] loss: 0.319\n",
      "---- Mean loss = 0.360 -------- Loss Val 488.487----\n",
      "====== Save New model ===== \n",
      "[tensor(545.2727, device='cuda:0'), tensor(501.7780, device='cuda:0'), tensor(488.4875, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[4,    50] loss: 0.398\n",
      "[4,   100] loss: 0.342\n",
      "[4,   150] loss: 0.364\n",
      "[4,   200] loss: 0.315\n",
      "[4,   250] loss: 0.349\n",
      "[4,   300] loss: 0.325\n",
      "[4,   350] loss: 0.317\n",
      "[4,   400] loss: 0.306\n",
      "---- Mean loss = 0.339 -------- Loss Val 474.029----\n",
      "====== Save New model ===== \n",
      "[tensor(545.2727, device='cuda:0'), tensor(501.7780, device='cuda:0'), tensor(488.4875, device='cuda:0'), tensor(474.0293, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[5,    50] loss: 0.356\n",
      "[5,   100] loss: 0.326\n",
      "[5,   150] loss: 0.334\n",
      "[5,   200] loss: 0.303\n",
      "[5,   250] loss: 0.333\n",
      "[5,   300] loss: 0.301\n",
      "[5,   350] loss: 0.294\n",
      "[5,   400] loss: 0.288\n",
      "---- Mean loss = 0.317 -------- Loss Val 464.201----\n",
      "====== Save New model ===== \n",
      "[tensor(545.2727, device='cuda:0'), tensor(501.7780, device='cuda:0'), tensor(488.4875, device='cuda:0'), tensor(474.0293, device='cuda:0'), tensor(464.2005, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[6,    50] loss: 0.331\n",
      "[6,   100] loss: 0.305\n",
      "[6,   150] loss: 0.326\n",
      "[6,   200] loss: 0.289\n",
      "[6,   250] loss: 0.307\n",
      "[6,   300] loss: 0.273\n",
      "[6,   350] loss: 0.267\n",
      "[6,   400] loss: 0.266\n",
      "---- Mean loss = 0.296 -------- Loss Val 466.923----\n",
      "=== Current lr: 0.000===\n",
      "[7,    50] loss: 0.322\n",
      "[7,   100] loss: 0.279\n",
      "[7,   150] loss: 0.296\n",
      "[7,   200] loss: 0.261\n",
      "[7,   250] loss: 0.288\n",
      "[7,   300] loss: 0.263\n",
      "[7,   350] loss: 0.265\n",
      "[7,   400] loss: 0.253\n",
      "---- Mean loss = 0.278 -------- Loss Val 452.490----\n",
      "====== Save New model ===== \n",
      "[tensor(545.2727, device='cuda:0'), tensor(501.7780, device='cuda:0'), tensor(488.4875, device='cuda:0'), tensor(474.0293, device='cuda:0'), tensor(464.2005, device='cuda:0'), tensor(466.9232, device='cuda:0'), tensor(452.4901, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[8,    50] loss: 0.288\n",
      "[8,   100] loss: 0.277\n",
      "[8,   150] loss: 0.279\n",
      "[8,   200] loss: 0.246\n",
      "[8,   250] loss: 0.271\n",
      "[8,   300] loss: 0.238\n",
      "[8,   350] loss: 0.238\n",
      "[8,   400] loss: 0.230\n",
      "---- Mean loss = 0.258 -------- Loss Val 445.384----\n",
      "====== Save New model ===== \n",
      "[tensor(545.2727, device='cuda:0'), tensor(501.7780, device='cuda:0'), tensor(488.4875, device='cuda:0'), tensor(474.0293, device='cuda:0'), tensor(464.2005, device='cuda:0'), tensor(466.9232, device='cuda:0'), tensor(452.4901, device='cuda:0'), tensor(445.3842, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[9,    50] loss: 0.268\n",
      "[9,   100] loss: 0.261\n",
      "[9,   150] loss: 0.268\n",
      "[9,   200] loss: 0.235\n",
      "[9,   250] loss: 0.255\n",
      "[9,   300] loss: 0.231\n",
      "[9,   350] loss: 0.224\n",
      "[9,   400] loss: 0.212\n",
      "---- Mean loss = 0.244 -------- Loss Val 446.470----\n",
      "=== Current lr: 0.000===\n",
      "[10,    50] loss: 0.250\n",
      "[10,   100] loss: 0.238\n",
      "[10,   150] loss: 0.246\n",
      "[10,   200] loss: 0.220\n",
      "[10,   250] loss: 0.243\n",
      "[10,   300] loss: 0.212\n",
      "[10,   350] loss: 0.206\n",
      "[10,   400] loss: 0.196\n",
      "---- Mean loss = 0.226 -------- Loss Val 447.378----\n",
      "=== Current lr: 0.000===\n",
      "[11,    50] loss: 0.253\n",
      "[11,   100] loss: 0.226\n",
      "[11,   150] loss: 0.227\n",
      "[11,   200] loss: 0.201\n",
      "[11,   250] loss: 0.216\n",
      "[11,   300] loss: 0.198\n",
      "[11,   350] loss: 0.205\n",
      "[11,   400] loss: 0.192\n",
      "---- Mean loss = 0.215 -------- Loss Val 441.007----\n",
      "====== Save New model ===== \n",
      "[tensor(545.2727, device='cuda:0'), tensor(501.7780, device='cuda:0'), tensor(488.4875, device='cuda:0'), tensor(474.0293, device='cuda:0'), tensor(464.2005, device='cuda:0'), tensor(466.9232, device='cuda:0'), tensor(452.4901, device='cuda:0'), tensor(445.3842, device='cuda:0'), tensor(446.4699, device='cuda:0'), tensor(447.3778, device='cuda:0'), tensor(441.0068, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[12,    50] loss: 0.226\n",
      "[12,   100] loss: 0.204\n",
      "[12,   150] loss: 0.216\n",
      "[12,   200] loss: 0.190\n",
      "[12,   250] loss: 0.216\n",
      "[12,   300] loss: 0.177\n",
      "[12,   350] loss: 0.173\n",
      "[12,   400] loss: 0.177\n",
      "---- Mean loss = 0.197 -------- Loss Val 444.069----\n",
      "=== Current lr: 0.000===\n",
      "[13,    50] loss: 0.211\n",
      "[13,   100] loss: 0.191\n",
      "[13,   150] loss: 0.213\n",
      "[13,   200] loss: 0.172\n",
      "[13,   250] loss: 0.201\n",
      "[13,   300] loss: 0.171\n",
      "[13,   350] loss: 0.162\n",
      "[13,   400] loss: 0.168\n",
      "---- Mean loss = 0.186 -------- Loss Val 439.881----\n",
      "====== Save New model ===== \n",
      "[tensor(545.2727, device='cuda:0'), tensor(501.7780, device='cuda:0'), tensor(488.4875, device='cuda:0'), tensor(474.0293, device='cuda:0'), tensor(464.2005, device='cuda:0'), tensor(466.9232, device='cuda:0'), tensor(452.4901, device='cuda:0'), tensor(445.3842, device='cuda:0'), tensor(446.4699, device='cuda:0'), tensor(447.3778, device='cuda:0'), tensor(441.0068, device='cuda:0'), tensor(444.0686, device='cuda:0'), tensor(439.8808, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[14,    50] loss: 0.192\n",
      "[14,   100] loss: 0.181\n",
      "[14,   150] loss: 0.202\n",
      "[14,   200] loss: 0.154\n",
      "[14,   250] loss: 0.188\n",
      "[14,   300] loss: 0.151\n",
      "[14,   350] loss: 0.175\n",
      "[14,   400] loss: 0.159\n",
      "---- Mean loss = 0.175 -------- Loss Val 452.140----\n",
      "=== Current lr: 0.000===\n",
      "[15,    50] loss: 0.173\n",
      "[15,   100] loss: 0.167\n",
      "[15,   150] loss: 0.164\n",
      "[15,   200] loss: 0.137\n",
      "[15,   250] loss: 0.161\n",
      "[15,   300] loss: 0.137\n",
      "[15,   350] loss: 0.131\n",
      "[15,   400] loss: 0.133\n",
      "---- Mean loss = 0.150 -------- Loss Val 443.839----\n",
      "=== Current lr: 0.000===\n",
      "Finished Training\n",
      "[1,    50] loss: 0.695\n",
      "[1,   100] loss: 0.632\n",
      "[1,   150] loss: 0.543\n",
      "[1,   200] loss: 0.513\n",
      "[1,   250] loss: 0.462\n",
      "[1,   300] loss: 0.425\n",
      "[1,   350] loss: 0.409\n",
      "[1,   400] loss: 0.435\n",
      "---- Mean loss = 0.514 -------- Loss Val 522.152----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[2,    50] loss: 0.447\n",
      "[2,   100] loss: 0.372\n",
      "[2,   150] loss: 0.390\n",
      "[2,   200] loss: 0.417\n",
      "[2,   250] loss: 0.371\n",
      "[2,   300] loss: 0.362\n",
      "[2,   350] loss: 0.367\n",
      "[2,   400] loss: 0.376\n",
      "---- Mean loss = 0.388 -------- Loss Val 489.493----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0'), tensor(489.4928, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[3,    50] loss: 0.405\n",
      "[3,   100] loss: 0.345\n",
      "[3,   150] loss: 0.354\n",
      "[3,   200] loss: 0.380\n",
      "[3,   250] loss: 0.329\n",
      "[3,   300] loss: 0.323\n",
      "[3,   350] loss: 0.334\n",
      "[3,   400] loss: 0.362\n",
      "---- Mean loss = 0.354 -------- Loss Val 477.637----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0'), tensor(489.4928, device='cuda:0'), tensor(477.6366, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[4,    50] loss: 0.380\n",
      "[4,   100] loss: 0.322\n",
      "[4,   150] loss: 0.317\n",
      "[4,   200] loss: 0.367\n",
      "[4,   250] loss: 0.308\n",
      "[4,   300] loss: 0.316\n",
      "[4,   350] loss: 0.309\n",
      "[4,   400] loss: 0.327\n",
      "---- Mean loss = 0.331 -------- Loss Val 466.124----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0'), tensor(489.4928, device='cuda:0'), tensor(477.6366, device='cuda:0'), tensor(466.1240, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[5,    50] loss: 0.352\n",
      "[5,   100] loss: 0.297\n",
      "[5,   150] loss: 0.299\n",
      "[5,   200] loss: 0.345\n",
      "[5,   250] loss: 0.291\n",
      "[5,   300] loss: 0.282\n",
      "[5,   350] loss: 0.294\n",
      "[5,   400] loss: 0.311\n",
      "---- Mean loss = 0.309 -------- Loss Val 455.275----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0'), tensor(489.4928, device='cuda:0'), tensor(477.6366, device='cuda:0'), tensor(466.1240, device='cuda:0'), tensor(455.2747, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[6,    50] loss: 0.341\n",
      "[6,   100] loss: 0.280\n",
      "[6,   150] loss: 0.269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,   200] loss: 0.323\n",
      "[6,   250] loss: 0.272\n",
      "[6,   300] loss: 0.271\n",
      "[6,   350] loss: 0.263\n",
      "[6,   400] loss: 0.297\n",
      "---- Mean loss = 0.290 -------- Loss Val 450.832----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0'), tensor(489.4928, device='cuda:0'), tensor(477.6366, device='cuda:0'), tensor(466.1240, device='cuda:0'), tensor(455.2747, device='cuda:0'), tensor(450.8319, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[7,    50] loss: 0.322\n",
      "[7,   100] loss: 0.257\n",
      "[7,   150] loss: 0.261\n",
      "[7,   200] loss: 0.300\n",
      "[7,   250] loss: 0.264\n",
      "[7,   300] loss: 0.255\n",
      "[7,   350] loss: 0.267\n",
      "[7,   400] loss: 0.275\n",
      "---- Mean loss = 0.275 -------- Loss Val 449.009----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0'), tensor(489.4928, device='cuda:0'), tensor(477.6366, device='cuda:0'), tensor(466.1240, device='cuda:0'), tensor(455.2747, device='cuda:0'), tensor(450.8319, device='cuda:0'), tensor(449.0093, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[8,    50] loss: 0.322\n",
      "[8,   100] loss: 0.253\n",
      "[8,   150] loss: 0.260\n",
      "[8,   200] loss: 0.280\n",
      "[8,   250] loss: 0.241\n",
      "[8,   300] loss: 0.224\n",
      "[8,   350] loss: 0.235\n",
      "[8,   400] loss: 0.253\n",
      "---- Mean loss = 0.259 -------- Loss Val 446.896----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0'), tensor(489.4928, device='cuda:0'), tensor(477.6366, device='cuda:0'), tensor(466.1240, device='cuda:0'), tensor(455.2747, device='cuda:0'), tensor(450.8319, device='cuda:0'), tensor(449.0093, device='cuda:0'), tensor(446.8961, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[9,    50] loss: 0.279\n",
      "[9,   100] loss: 0.223\n",
      "[9,   150] loss: 0.229\n",
      "[9,   200] loss: 0.264\n",
      "[9,   250] loss: 0.231\n",
      "[9,   300] loss: 0.212\n",
      "[9,   350] loss: 0.224\n",
      "[9,   400] loss: 0.247\n",
      "---- Mean loss = 0.239 -------- Loss Val 443.449----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0'), tensor(489.4928, device='cuda:0'), tensor(477.6366, device='cuda:0'), tensor(466.1240, device='cuda:0'), tensor(455.2747, device='cuda:0'), tensor(450.8319, device='cuda:0'), tensor(449.0093, device='cuda:0'), tensor(446.8961, device='cuda:0'), tensor(443.4491, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[10,    50] loss: 0.275\n",
      "[10,   100] loss: 0.222\n",
      "[10,   150] loss: 0.222\n",
      "[10,   200] loss: 0.255\n",
      "[10,   250] loss: 0.211\n",
      "[10,   300] loss: 0.199\n",
      "[10,   350] loss: 0.223\n",
      "[10,   400] loss: 0.212\n",
      "---- Mean loss = 0.227 -------- Loss Val 442.709----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0'), tensor(489.4928, device='cuda:0'), tensor(477.6366, device='cuda:0'), tensor(466.1240, device='cuda:0'), tensor(455.2747, device='cuda:0'), tensor(450.8319, device='cuda:0'), tensor(449.0093, device='cuda:0'), tensor(446.8961, device='cuda:0'), tensor(443.4491, device='cuda:0'), tensor(442.7086, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[11,    50] loss: 0.241\n",
      "[11,   100] loss: 0.209\n",
      "[11,   150] loss: 0.195\n",
      "[11,   200] loss: 0.231\n",
      "[11,   250] loss: 0.199\n",
      "[11,   300] loss: 0.183\n",
      "[11,   350] loss: 0.212\n",
      "[11,   400] loss: 0.213\n",
      "---- Mean loss = 0.210 -------- Loss Val 450.039----\n",
      "=== Current lr: 0.000===\n",
      "[12,    50] loss: 0.227\n",
      "[12,   100] loss: 0.195\n",
      "[12,   150] loss: 0.178\n",
      "[12,   200] loss: 0.219\n",
      "[12,   250] loss: 0.187\n",
      "[12,   300] loss: 0.170\n",
      "[12,   350] loss: 0.183\n",
      "[12,   400] loss: 0.193\n",
      "---- Mean loss = 0.194 -------- Loss Val 443.574----\n",
      "=== Current lr: 0.000===\n",
      "[13,    50] loss: 0.222\n",
      "[13,   100] loss: 0.166\n",
      "[13,   150] loss: 0.174\n",
      "[13,   200] loss: 0.200\n",
      "[13,   250] loss: 0.167\n",
      "[13,   300] loss: 0.156\n",
      "[13,   350] loss: 0.168\n",
      "[13,   400] loss: 0.182\n",
      "---- Mean loss = 0.179 -------- Loss Val 454.446----\n",
      "=== Current lr: 0.000===\n",
      "[14,    50] loss: 0.204\n",
      "[14,   100] loss: 0.158\n",
      "[14,   150] loss: 0.155\n",
      "[14,   200] loss: 0.165\n",
      "[14,   250] loss: 0.148\n",
      "[14,   300] loss: 0.138\n",
      "[14,   350] loss: 0.154\n",
      "[14,   400] loss: 0.149\n",
      "---- Mean loss = 0.159 -------- Loss Val 431.780----\n",
      "====== Save New model ===== \n",
      "[tensor(522.1523, device='cuda:0'), tensor(489.4928, device='cuda:0'), tensor(477.6366, device='cuda:0'), tensor(466.1240, device='cuda:0'), tensor(455.2747, device='cuda:0'), tensor(450.8319, device='cuda:0'), tensor(449.0093, device='cuda:0'), tensor(446.8961, device='cuda:0'), tensor(443.4491, device='cuda:0'), tensor(442.7086, device='cuda:0'), tensor(450.0388, device='cuda:0'), tensor(443.5735, device='cuda:0'), tensor(454.4461, device='cuda:0'), tensor(431.7801, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[15,    50] loss: 0.200\n",
      "[15,   100] loss: 0.152\n",
      "[15,   150] loss: 0.160\n",
      "[15,   200] loss: 0.167\n",
      "[15,   250] loss: 0.145\n",
      "[15,   300] loss: 0.138\n",
      "[15,   350] loss: 0.144\n",
      "[15,   400] loss: 0.152\n",
      "---- Mean loss = 0.157 -------- Loss Val 432.478----\n",
      "=== Current lr: 0.000===\n",
      "Finished Training\n",
      "[1,    50] loss: 0.706\n",
      "[1,   100] loss: 0.598\n",
      "[1,   150] loss: 0.535\n",
      "[1,   200] loss: 0.504\n",
      "[1,   250] loss: 0.497\n",
      "[1,   300] loss: 0.474\n",
      "[1,   350] loss: 0.425\n",
      "[1,   400] loss: 0.405\n",
      "---- Mean loss = 0.518 -------- Loss Val 516.858----\n",
      "====== Save New model ===== \n",
      "[tensor(516.8582, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[2,    50] loss: 0.399\n",
      "[2,   100] loss: 0.373\n",
      "[2,   150] loss: 0.385\n",
      "[2,   200] loss: 0.400\n",
      "[2,   250] loss: 0.409\n",
      "[2,   300] loss: 0.417\n",
      "[2,   350] loss: 0.350\n",
      "[2,   400] loss: 0.356\n",
      "---- Mean loss = 0.386 -------- Loss Val 481.670----\n",
      "====== Save New model ===== \n",
      "[tensor(516.8582, device='cuda:0'), tensor(481.6696, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[3,    50] loss: 0.370\n",
      "[3,   100] loss: 0.342\n",
      "[3,   150] loss: 0.351\n",
      "[3,   200] loss: 0.372\n",
      "[3,   250] loss: 0.383\n",
      "[3,   300] loss: 0.396\n",
      "[3,   350] loss: 0.327\n",
      "[3,   400] loss: 0.321\n",
      "---- Mean loss = 0.358 -------- Loss Val 465.960----\n",
      "====== Save New model ===== \n",
      "[tensor(516.8582, device='cuda:0'), tensor(481.6696, device='cuda:0'), tensor(465.9602, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[4,    50] loss: 0.338\n",
      "[4,   100] loss: 0.336\n",
      "[4,   150] loss: 0.319\n",
      "[4,   200] loss: 0.342\n",
      "[4,   250] loss: 0.351\n",
      "[4,   300] loss: 0.362\n",
      "[4,   350] loss: 0.304\n",
      "[4,   400] loss: 0.309\n",
      "---- Mean loss = 0.333 -------- Loss Val 455.807----\n",
      "====== Save New model ===== \n",
      "[tensor(516.8582, device='cuda:0'), tensor(481.6696, device='cuda:0'), tensor(465.9602, device='cuda:0'), tensor(455.8075, device='cuda:0')]\n",
      "=== Current lr: 0.000===\n",
      "[5,    50] loss: 0.336\n",
      "[5,   100] loss: 0.322\n",
      "[5,   150] loss: 0.302\n",
      "[5,   200] loss: 0.333\n",
      "[5,   250] loss: 0.327\n",
      "[5,   300] loss: 0.332\n",
      "[5,   350] loss: 0.291\n",
      "[5,   400] loss: 0.281\n",
      "---- Mean loss = 0.315 ----"
     ]
    }
   ],
   "source": [
    "for numero in range(3):\n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    lr = 0.0001\n",
    "    kernel_sizes = [3, 4, 5]\n",
    "    name = 'bestModel/kim/' + str(numero) + '_2'\n",
    "    cnn = CnnKim(matrix_weight=torch.from_numpy(weight_matrix).float(),\n",
    "                 len_sentence=max_length,\n",
    "                 kernel_sizes=kernel_sizes)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cnn.to(device)\n",
    "    # Create trainer\n",
    "    trainer = list(DataLoader(DataSet(X_train, Y_train), batch_size=batch_size, shuffle=True))\n",
    "    valider = list(DataLoader(DataSet(X_val, Y_val), batch_size=1, shuffle=False))\n",
    "    # train our model\n",
    "    train_callback(cnn,\n",
    "                   trainer,\n",
    "                   valider,\n",
    "                   name_save=name,\n",
    "                   w=None,\n",
    "                   weight_decay=1e-4,\n",
    "                   epochs=epochs,\n",
    "                   lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "enormous-shadow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CnnKim(\n",
       "  (embbeding_1): Embedding(16599, 200)\n",
       "  (embbeding_2): Embedding(16599, 200)\n",
       "  (block): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(2, 100, kernel_size=(3, 200), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Flatten()\n",
       "      (3): MaxPool1d(kernel_size=22188, stride=22188, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(2, 100, kernel_size=(4, 200), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Flatten()\n",
       "      (3): MaxPool1d(kernel_size=22188, stride=22188, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(2, 100, kernel_size=(5, 200), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Flatten()\n",
       "      (3): MaxPool1d(kernel_size=22188, stride=22188, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numero = 2\n",
    "kernel_sizes = [3, 4, 5]\n",
    "name = 'bestModel/kim/' + str(numero) + '_2.pth'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn = CnnKim(matrix_weight=torch.from_numpy(weight_matrix).float(),\n",
    "             len_sentence=max_length, kernel_sizes=kernel_sizes)\n",
    "cnn.load_state_dict(torch.load(name))\n",
    "cnn.to(device)\n",
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "twenty-timer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8468979319546365 \n",
      " F1: 0.81877545602057 \n",
      " precision : 0.8498897667515934 \n",
      " recall : 0.8026675888107903 \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = iter(DataLoader(DataSet(X_test, Y_test),\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False))\n",
    "    y_pred = get_all_preds(cnn, prediction_loader)\n",
    "proba, predicted = torch.max(y_pred, 1)\n",
    "new_score(Y_test.numpy(), predicted.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sensitive-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "innovative-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1, 2, 3, 4],\n",
    "                  [1, 3, 4, 10]]], dtype=torch.float32)\n",
    "layer = nn.AdaptiveMaxPool1d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "voluntary-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "historical-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((2,3,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ultimate-april",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pad_packed_sequence() got multiple values for argument 'batch_first'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fc6643e18442>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: pad_packed_sequence() got multiple values for argument 'batch_first'"
     ]
    }
   ],
   "source": [
    "X = nn.utils.rnn.pad_packed_sequence(X, lengths=[3, 3], batch_first=False, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-brain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
